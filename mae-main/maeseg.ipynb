{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from util.diceloss import SoftDiceLoss\n",
    "from util.diceloss import DiceLoss\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "from util.pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "import seg3d_2dencoder\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from swin_transformer import *\n",
    "from email.mime import image\n",
    "from turtle import forward\n",
    "from util.diceloss import DiceLoss\n",
    "from einops import rearrange\n",
    "class SegVit3D(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder: 预训练2dmae的encoder参数(冻结权重)\n",
    "    decoder: 采用3D滑窗transformer_decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224,img_deep=160, patch_size=16, in_chans=1,embed_dim=256,\n",
    "                 decoder_embed_dim=256,decoder_depth=[2,2,2,2],decoder_num_heads=[4,8,16,32],\n",
    "                 encoder:nn.Module=None,encoder_finetune:str=None,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False,\n",
    "                 window_size = 4, shift_size = 1,\n",
    "                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 fused_window_process=False,\n",
    "                 lossforpatch = True,\n",
    "                 ):\n",
    "        r\"\"\"SegVit3D.\n",
    "        Args:\n",
    "            img_size(int):the size of one slide of 3D subject\n",
    "            img_deep(int):the slides number of 3D subject\n",
    "            in_chans(int):the channel of 3D subject\n",
    "            embed_dim (int):the dim of encoder patch_embed\n",
    "            decoder_dim(int):the dim of decoder patch_embed\n",
    "            decoder_depth(int):the decoder-block number \n",
    "            decoder_num_heads (int): Number of decoder attention heads.\n",
    "            encoder(nn.Module):mae encoder(pretrain)\n",
    "            encoder_fine_tune(str):pretrain model path\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            window_size (int): Window size.\n",
    "            shift_size (int): Shift size for SW-MSA.\n",
    "            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop (float, optional): Dropout rate. Default: 0.0\n",
    "            attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "            drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "            act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "            fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "            lossforpatch:choose target->patch or pred->image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #-----------------------------------------------------\n",
    "        self.img_size = img_size\n",
    "        self.norm = norm_layer(decoder_embed_dim)\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.img_deep = img_deep\n",
    "        \n",
    "        # MaeSeg3D encoder\n",
    "        self.encoder = encoder\n",
    "        # ---------------------------------------------------------------------\n",
    "        # MaeSeg3D decoder \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, sum(decoder_depth))]  # stochastic depth decay rule\n",
    "\n",
    "        self.num_layers = len(decoder_depth)\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(decoder_embed_dim),\n",
    "                               input_resolution=(int((img_size//patch_size)**2) ,\n",
    "                                                 img_deep ),\n",
    "                               depth=decoder_depth[i_layer],\n",
    "                               num_heads=decoder_num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop, attn_drop=attn_drop,\n",
    "                               drop_path=dpr[sum(decoder_depth[:i_layer]):sum(decoder_depth[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample= None,\n",
    "                               use_checkpoint=False,\n",
    "                               fused_window_process=fused_window_process)\n",
    "            self.layers.append(layer)\n",
    "        self.apply(self._init_weights)\n",
    "        self.lossforpatch = lossforpatch\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "\n",
    "\n",
    "    def encoder_forward(self,x):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        x_in:(N,1,D,H,W) #(1,1,160,224,224)\n",
    "        return:\n",
    "        x_latent:(N,L,patch_size**2*1) #(1,196*160,256)\n",
    "        \"\"\"\n",
    "        N,C,D,H,W = x.shape()\n",
    "        \n",
    "        x = x.squeeze(0).permute(1,0,2,3)\n",
    "        x_latent = self.encoder(x)\n",
    "        return x_latent\n",
    "        \n",
    "\n",
    "    # x_latent.shape = (160,196,256)\n",
    "\n",
    "    def decoder_forward(self,x):\n",
    "        \"\"\"\n",
    "        x_in:(B,patch_num*img_deep,256)  #(B,196*160,256)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.norm(x) # B L C\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self,pred,target):\n",
    "        \"\"\"\n",
    "        target:[B,1,H,W,C]\n",
    "        pred:[B,L,256]\n",
    "        \"\"\"\n",
    "        LOSS = DiceLoss()\n",
    "        if self.lossforpatch:\n",
    "            target = self.patchify3D(target)  #[N,196,256]\n",
    "            assert pred.shape == target.shape\n",
    "            loss = LOSS(pred,target)\n",
    "        else:\n",
    "            pred = self.unpatchify3D(pred)  #[N,1,224,224]\n",
    "            assert pred.shape == target.shape\n",
    "            loss = LOSS(pred,target)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self,imgs,label):\n",
    "        latent = self.encoder_forward(imgs)\n",
    "        pred = self.decoder_forward(latent)  # [N, L, p*p*1] (N,196,256)\n",
    "        loss = self.forward_loss(pred, label)\n",
    "        return loss, pred\n",
    "        \n",
    "\n",
    "    def patchify3D(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 1, T, H, W)\n",
    "        x: (N, L, patch_size**2 *1 *temp_stride)\n",
    "        \"\"\"\n",
    "        x = rearrange(imgs, 'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c', p0=1, p1=self.patch_size, p2=self.patch_size)\n",
    "        x = rearrange(x, 'b n p c -> b n (p c)')\n",
    "        return x\n",
    "\n",
    "    def unpatchify3D(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *1 *temp_stride)\n",
    "        imgs: (N, 1, T, H, W)\n",
    "        \"\"\"\n",
    "        x = rearrange(x, 'b (t h w) (p0 p1 p2 c) -> b c (t p0) (h p1) (w p2)', p1=self.patch_size, p2=self.patch_size, c=self.in_chans, h=int(self.img_size//self.patch_size), w=int(self.img_size//self.patch_size))\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('ZH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d66a6644cd8b9bdcc5b69d4f758b89eb8a6590d2d16af97b0232df0dbc0ae54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
